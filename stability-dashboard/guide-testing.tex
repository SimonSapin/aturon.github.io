\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={The Rust Testing Guide},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

\title{The Rust Testing Guide}

\begin{document}
\maketitle

0.12.0-pre (2c50add48 2014-07-27 23:03:52 -0700)

Copyright © 2011-2014 The Rust Project Developers. Licensed under the
\href{http://www.apache.org/licenses/LICENSE-2.0}{Apache License,
Version 2.0} or the \href{http://opensource.org/licenses/MIT}{MIT
license}, at your option.

This file may not be copied, modified, or distributed except according
to those terms.

{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{3}
\tableofcontents
}
\section{Quick start}\label{quick-start}

To create test functions, add a \texttt{\#{[}test{]}} attribute like
this:

\begin{verbatim}
fn return_two() -> int {
    2
}

#[test]
fn return_two_test() {
    let x = return_two();
    assert!(x == 2);
}
\end{verbatim}

To run these tests, compile with \texttt{rustc -\/-test} and run the
resulting binary:

\begin{verbatim}
$ rustc --test foo.rs
$ ./foo
running 1 test
test return_two_test ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured
\end{verbatim}

\texttt{rustc foo.rs} will \emph{not} compile the tests, since
\texttt{\#{[}test{]}} implies \texttt{\#{[}cfg(test){]}}. The
\texttt{-\/-test} flag to \texttt{rustc} implies \texttt{-\/-cfg test}.

\section{Unit testing in Rust}\label{unit-testing-in-rust}

Rust has built in support for simple unit testing. Functions can be
marked as unit tests using the \texttt{test} attribute.

\begin{verbatim}
#[test]
fn return_none_if_empty() {
    // ... test code ...
}
\end{verbatim}

A test function's signature must have no arguments and no return value.
To run the tests in a crate, it must be compiled with the
\texttt{-\/-test} flag:
\texttt{rustc myprogram.rs -\/-test -o myprogram-tests}. Running the
resulting executable will run all the tests in the crate. A test is
considered successful if its function returns; if the task running the
test fails, through a call to \texttt{fail!}, a failed \texttt{assert},
or some other (\texttt{assert\_eq}, \ldots{}) means, then the test
fails.

When compiling a crate with the \texttt{-\/-test} flag
\texttt{-\/-cfg test} is also implied, so that tests can be
conditionally compiled.

\begin{verbatim}
#[cfg(test)]
mod tests {
    #[test]
    fn return_none_if_empty() {
      // ... test code ...
    }
}
\end{verbatim}

Additionally \texttt{\#{[}test{]}} items behave as if they also have the
\texttt{\#{[}cfg(test){]}} attribute, and will not be compiled when the
\texttt{-\/-test} flag is not used.

Tests that should not be run can be annotated with the \texttt{ignore}
attribute. The existence of these tests will be noted in the test runner
output, but the test will not be run. Tests can also be ignored by
configuration so, for example, to ignore a test on windows you can write
\texttt{\#{[}ignore(cfg(target\_os = "win32")){]}}.

Tests that are intended to fail can be annotated with the
\texttt{should\_fail} attribute. The test will be run, and if it causes
its task to fail then the test will be counted as successful; otherwise
it will be counted as a failure. For example:

\begin{verbatim}
#[test]
#[should_fail]
fn test_out_of_bounds_failure() {
    let v: &[int] = [];
    v[0];
}
\end{verbatim}

A test runner built with the \texttt{-\/-test} flag supports a limited
set of arguments to control which tests are run:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  the first free argument passed to a test runner is interpreted as a
  regular expression (\href{regex/index.html\#syntax}{syntax reference})
  and is used to narrow down the set of tests being run. Note: a plain
  string is a valid regular expression that matches itself.
\item
  the \texttt{-\/-ignored} flag tells the test runner to run only tests
  with the \texttt{ignore} attribute.
\end{itemize}

\subsection{Parallelism}\label{parallelism}

By default, tests are run in parallel, which can make interpreting
failure output difficult. In these cases you can set the
\texttt{RUST\_TEST\_TASKS} environment variable to 1 to make the tests
run sequentially.

\subsection{Examples}\label{examples}

\subsubsection{Typical test run}\label{typical-test-run}

\begin{verbatim}
$ mytests

running 30 tests
running driver::tests::mytest1 ... ok
running driver::tests::mytest2 ... ignored
... snip ...
running driver::tests::mytest30 ... ok

result: ok. 28 passed; 0 failed; 2 ignored
\end{verbatim}

\subsubsection{Test run with failures}\label{test-run-with-failures}

\begin{verbatim}
$ mytests

running 30 tests
running driver::tests::mytest1 ... ok
running driver::tests::mytest2 ... ignored
... snip ...
running driver::tests::mytest30 ... FAILED

result: FAILED. 27 passed; 1 failed; 2 ignored
\end{verbatim}

\subsubsection{Running ignored tests}\label{running-ignored-tests}

\begin{verbatim}
$ mytests --ignored

running 2 tests
running driver::tests::mytest2 ... failed
running driver::tests::mytest10 ... ok

result: FAILED. 1 passed; 1 failed; 0 ignored
\end{verbatim}

\subsubsection{Running a subset of
tests}\label{running-a-subset-of-tests}

Using a plain string:

\begin{verbatim}
$ mytests mytest23

running 1 tests
running driver::tests::mytest23 ... ok

result: ok. 1 passed; 0 failed; 0 ignored
\end{verbatim}

Using some regular expression features:

\begin{verbatim}
$ mytests 'mytest[145]'

running 13 tests
running driver::tests::mytest1 ... ok
running driver::tests::mytest4 ... ok
running driver::tests::mytest5 ... ok
running driver::tests::mytest10 ... ignored
... snip ...
running driver::tests::mytest19 ... ok

result: ok. 13 passed; 0 failed; 1 ignored
\end{verbatim}

\section{Microbenchmarking}\label{microbenchmarking}

The test runner also understands a simple form of benchmark execution.
Benchmark functions are marked with the \texttt{\#{[}bench{]}}
attribute, rather than \texttt{\#{[}test{]}}, and have a different form
and meaning. They are compiled along with \texttt{\#{[}test{]}}
functions when a crate is compiled with \texttt{-\/-test}, but they are
not run by default. To run the benchmark component of your testsuite,
pass \texttt{-\/-bench} to the compiled test runner.

The type signature of a benchmark function differs from a unit test: it
takes a mutable reference to type \texttt{test::Bencher}. Inside the
benchmark function, any time-variable or ``setup'' code should execute
first, followed by a call to \texttt{iter} on the benchmark harness,
passing a closure that contains the portion of the benchmark you wish to
actually measure the per-iteration speed of.

For benchmarks relating to processing/generating data, one can set the
\texttt{bytes} field to the number of bytes consumed/produced in each
iteration; this will used to show the throughput of the benchmark. This
must be the amount used in each iteration, \emph{not} the total amount.

For example:

\begin{verbatim}
extern crate test;

use test::Bencher;

#[bench]
fn bench_sum_1024_ints(b: &mut Bencher) {
    let v = Vec::from_fn(1024, |n| n);
    b.iter(|| v.iter().fold(0, |old, new| old + *new));
}

#[bench]
fn initialise_a_vector(b: &mut Bencher) {
    b.iter(|| Vec::from_elem(1024, 0u64));
    b.bytes = 1024 * 8;
}
\end{verbatim}

The benchmark runner will calibrate measurement of the benchmark
function to run the \texttt{iter} block ``enough'' times to get a
reliable measure of the per-iteration speed.

Advice on writing benchmarks:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Move setup code outside the \texttt{iter} loop; only put the part you
  want to measure inside
\item
  Make the code do ``the same thing'' on each iteration; do not
  accumulate or change state
\item
  Make the outer function idempotent too; the benchmark runner is likely
  to run it many times
\item
  Make the inner \texttt{iter} loop short and fast so benchmark runs are
  fast and the calibrator can adjust the run-length at fine resolution
\item
  Make the code in the \texttt{iter} loop do something simple, to assist
  in pinpointing performance improvements (or regressions)
\end{itemize}

To run benchmarks, pass the \texttt{-\/-bench} flag to the compiled
test-runner. Benchmarks are compiled-in but not executed by default.

\begin{verbatim}
$ rustc mytests.rs -O --test
$ mytests --bench

running 2 tests
test bench_sum_1024_ints ... bench: 709 ns/iter (+/- 82)
test initialise_a_vector ... bench: 424 ns/iter (+/- 99) = 19320 MB/s

test result: ok. 0 passed; 0 failed; 0 ignored; 2 measured
\end{verbatim}

\subsection{Benchmarks and the
optimizer}\label{benchmarks-and-the-optimizer}

Benchmarks compiled with optimizations activated can be dramatically
changed by the optimizer so that the benchmark is no longer benchmarking
what one expects. For example, the compiler might recognize that some
calculation has no external effects and remove it entirely.

\begin{verbatim}
extern crate test;
use test::Bencher;

#[bench]
fn bench_xor_1000_ints(b: &mut Bencher) {
    b.iter(|| {
        range(0u, 1000).fold(0, |old, new| old ^ new);
    });
}
\end{verbatim}

gives the following results

\begin{verbatim}
running 1 test
test bench_xor_1000_ints ... bench:         0 ns/iter (+/- 0)

test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured
\end{verbatim}

The benchmarking runner offers two ways to avoid this. Either, the
closure that the \texttt{iter} method receives can return an arbitrary
value which forces the optimizer to consider the result used and ensures
it cannot remove the computation entirely. This could be done for the
example above by adjusting the \texttt{bh.iter} call to

\begin{verbatim}
# struct X; impl X { fn iter<T>(&self, _: || -> T) {} } let b = X;
b.iter(|| {
    // note lack of `;` (could also use an explicit `return`).
    range(0u, 1000).fold(0, |old, new| old ^ new)
});
\end{verbatim}

Or, the other option is to call the generic \texttt{test::black\_box}
function, which is an opaque ``black box'' to the optimizer and so
forces it to consider any argument as used.

\begin{verbatim}
extern crate test;

# fn main() {
# struct X; impl X { fn iter<T>(&self, _: || -> T) {} } let b = X;
b.iter(|| {
    test::black_box(range(0u, 1000).fold(0, |old, new| old ^ new));
});
# }
\end{verbatim}

Neither of these read or modify the value, and are very cheap for small
values. Larger values can be passed indirectly to reduce overhead (e.g.
\texttt{black\_box(\&huge\_struct)}).

Performing either of the above changes gives the following benchmarking
results

\begin{verbatim}
running 1 test
test bench_xor_1000_ints ... bench:       375 ns/iter (+/- 148)

test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured
\end{verbatim}

However, the optimizer can still modify a testcase in an undesirable
manner even when using either of the above. Benchmarks can be checked by
hand by looking at the output of the compiler using the
\texttt{-\/-emit=ir} (for LLVM IR), \texttt{-\/-emit=asm} (for assembly)
or compiling normally and using any method for examining object code.

\subsection{Saving and ratcheting
metrics}\label{saving-and-ratcheting-metrics}

When running benchmarks or other tests, the test runner can record
per-test ``metrics''. Each metric is a scalar \texttt{f64} value, plus a
noise value which represents uncertainty in the measurement. By default,
all \texttt{\#{[}bench{]}} benchmarks are recorded as metrics, which can
be saved as JSON in an external file for further reporting.

In addition, the test runner supports \emph{ratcheting} against a
metrics file. Ratcheting is like saving metrics, except that after each
run, if the output file already exists the results of the current run
are compared against the contents of the existing file, and any
regression \emph{causes the testsuite to fail}. If the comparison passes
-- if all metrics stayed the same (within noise) or improved -- then the
metrics file is overwritten with the new values. In this way, a metrics
file in your workspace can be used to ensure your work does not regress
performance.

Test runners take 3 options that are relevant to metrics:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \texttt{-\/-save-metrics=\textless{}file.json\textgreater{}} will save
  the metrics from a test run to \texttt{file.json}
\item
  \texttt{-\/-ratchet-metrics=\textless{}file.json\textgreater{}} will
  ratchet the metrics against the \texttt{file.json}
\item
  \texttt{-\/-ratchet-noise-percent=N} will override the noise
  measurements in \texttt{file.json}, and consider a metric change less
  than \texttt{N\%} to be noise. This can be helpful if you are testing
  in a noisy environment where the benchmark calibration loop cannot
  acquire a clear enough signal.
\end{itemize}

\end{document}
